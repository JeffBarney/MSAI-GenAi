{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FurnitureGen: AI-Powered Interior Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeff Barney, MSAI-495: Generative AI Image Project\n",
    "\n",
    "# Questions:\n",
    "* How do you save off your model after training?\n",
    "* Do you save off the preprocessed data as well? Or is that low enough lift that you run that each session?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Goal / Case Statement\n",
    "Enable fast, iterative interior design by allowing users to vizualize any room in a different style. \n",
    "\n",
    "### Data\n",
    "* I downloaded my dataset from [Kaggle](https://www.kaggle.com/datasets/stepanyarullin/interior-design-styles) \n",
    "* The dataset consists of a collection of interior design images (~1,000 per style) scraped from Houzz.com\n",
    "* The data set contains 14,876 images in the training set, and 3,729 in the test set\n",
    "* All images are 320x320 and in the jpg file format\n",
    "* Each different style is listed below with an accompanying sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transitional</td>\n",
       "      <td><img src=\"../image-project-data/test/transitional/transitional_354.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>industrial</td>\n",
       "      <td><img src=\"../image-project-data/test/industrial/industrial_846.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shabby-chic-style</td>\n",
       "      <td><img src=\"../image-project-data/test/shabby-chic-style/shabby-chic-style_495.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asian</td>\n",
       "      <td><img src=\"../image-project-data/test/asian/asian_638.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>victorian</td>\n",
       "      <td><img src=\"../image-project-data/test/victorian/victorian_880.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coastal</td>\n",
       "      <td><img src=\"../image-project-data/test/coastal/coastal_693.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>southwestern</td>\n",
       "      <td><img src=\"../image-project-data/test/southwestern/southwestern_177.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>craftsman</td>\n",
       "      <td><img src=\"../image-project-data/test/craftsman/craftsman_497.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>contemporary</td>\n",
       "      <td><img src=\"../image-project-data/test/contemporary/contemporary_58.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>scandinavian</td>\n",
       "      <td><img src=\"../image-project-data/test/scandinavian/scandinavian_476.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>french-country</td>\n",
       "      <td><img src=\"../image-project-data/test/french-country/french-country_831.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mediterranean</td>\n",
       "      <td><img src=\"../image-project-data/test/mediterranean/mediterranean_87.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rustic</td>\n",
       "      <td><img src=\"../image-project-data/test/rustic/rustic_928.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>farmhouse</td>\n",
       "      <td><img src=\"../image-project-data/test/farmhouse/farmhouse_486.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mid-century-modern</td>\n",
       "      <td><img src=\"../image-project-data/test/mid-century-modern/mid-century-modern_686.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td><img src=\"../image-project-data/test/traditional/traditional_809.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eclectic</td>\n",
       "      <td><img src=\"../image-project-data/test/eclectic/eclectic_142.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tropical</td>\n",
       "      <td><img src=\"../image-project-data/test/tropical/tropical_609.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>modern</td>\n",
       "      <td><img src=\"../image-project-data/test/modern/modern_603.jpg\" width=\"320\"></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "root_dir = '../image-project-data'\n",
    "test_dir = root_dir + '/test'\n",
    "\n",
    "# Fetch the styles from our csv of labels\n",
    "raw_labels = pd.read_csv(root_dir + '/test_labels.csv')\n",
    "styles = raw_labels['style'].dropna().unique()\n",
    "styles_sorted = sorted(styles)\n",
    "\n",
    "# Build a list of dicts with a style and image sample\n",
    "rows = []\n",
    "for style in styles:\n",
    "    img_dir_path = os.path.join(test_dir, style)\n",
    "    img_file_name = next((img.name for img in os.scandir(img_dir_path) if img.is_file()), None)\n",
    "\n",
    "    img_tag = f'<img src=\"{os.path.join(img_dir_path, img_file_name)}\" width=\"320\">'\n",
    "    rows.append({\n",
    "        'Label': style,\n",
    "        'Image': img_tag\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "HTML(df.to_html(escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preload all the data so that the cpu isn't the bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Load and Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3729, 360, 360, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(imgs: np.ndarray) -> np.ndarray:\n",
    "   \"\"\"\n",
    "   Normalize \n",
    "\n",
    "   Args:\n",
    "      imgs (numpy.ndarray): Images to preprocess.\n",
    "   Returns:\n",
    "      imgs (numpy.ndarray): Preprocessed images.\n",
    "   \"\"\"\n",
    "   imgs = imgs.astype(\"float32\") / 255.0\n",
    "   imgs = np.expand_dims(imgs, -1)\n",
    "   return imgs\n",
    "\n",
    "\n",
    "def load_images_to_array(is_training=False):\n",
    "    root_dir = '../image-project-data/' + ('train' if is_training else 'test')\n",
    "    imgs = []\n",
    "    for path in Path(root_dir).rglob('*'):\n",
    "        if path.suffix.lower() == '.jpg':\n",
    "            with Image.open(path) as img:\n",
    "                imgs.append(np.array(img))\n",
    "    if not imgs:\n",
    "        return np.empty((0,))  # no images found\n",
    "    return np.stack(imgs, axis=0)\n",
    "\n",
    "data = load_images_to_array()\n",
    "data = preprocess(data)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Build the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend, layers, models\n",
    "\n",
    "EMBEDDING_DIM = 2\n",
    "IMAGE_SIZE = 320\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# You might want to add more layers and then some other things between the layers?\n",
    "encoder_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name=\"encoder_input\")\n",
    "x = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\n",
    "x = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "shape_before_flattening = backend.int_shape(x)[1:]\n",
    "x = layers.Flatten()(x)\n",
    "z_mean = layers.Dense(EMBEDDING_DIM, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(EMBEDDING_DIM, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Build the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = layers.Input(shape=(EMBEDDING_DIM,), name=\"decoder_input\")\n",
    "x = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "x = layers.Reshape(shape_before_flattening)(x)\n",
    "# You might want to add more layers and then some other things between the layers?\n",
    "x = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x) \n",
    "x = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "decoder_output = layers.Conv2D( 1, (3, 3), strides=1, activation=\"sigmoid\", padding=\"same\", name=\"decoder_output\")(x)\n",
    "decoder = models.Model(decoder_input, decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Combine the Encoder and Decoder into one VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import metrics, optimizers\n",
    "\n",
    "class VAE(models.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [ self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker, ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Call the model on a particular input.\"\"\"\n",
    "        z_mean, z_log_var, z = encoder(inputs)\n",
    "        reconstruction = decoder(z)\n",
    "        return z_mean, z_log_var, reconstruction\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"Step run during training.\"\"\"\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, reconstruction = self(data)\n",
    "            reconstruction_loss = tf.reduce_mean( BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3)))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        \"\"\"Step run during validation.\"\"\"\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_var, reconstruction = self(data)\n",
    "        reconstruction_loss = tf.reduce_mean(BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3)))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis=1))\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        return { \"loss\": total_loss, \"reconstruction_loss\": reconstruction_loss, \"kl_loss\": kl_loss, }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Create an instance of the VAE model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=1\n",
    "BATCH_SIZE=10\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "optimizer = optimizers.Adam(learning_rate=0.0005)\n",
    "vae.compile(optimizer=optimizer)\n",
    "history = vae.fit(data, data, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(data, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Visualize the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - Test the model for data loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some images and pass them through the full flow to visualize the data loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 - Calculate vectors for each interior design style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a latent vector for all images in our data set\n",
    "\n",
    "# Compute a latent vector for each style\n",
    "style_vector=mean(latent_vectors_style)−mean(all_latent_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 - Visualize rooms with a different style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table for each style that shows a before and after of a transformation to some other style\n",
    "new_vector =old_vector + alpha*feature_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
